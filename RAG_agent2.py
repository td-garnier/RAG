import os
import sys 
from dotenv import load_dotenv

# üÜï --- IMPORTS CHAINLIT ---
import chainlit as cl
# --- FIN IMPORTS CHAINLIT ---

# üÜï --- IMPORTS PHOENIX & INSTRUMENTATION ---
import phoenix as ph
from phoenix.otel import register
from openinference.instrumentation.langchain import LangChainInstrumentor 
from opentelemetry import trace
from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
    OpenAIModel,
    run_evals,
)
import pandas as pd 
# --- FIN IMPORTS PHOENIX ---

# LangChain/LLM/RAG/Agents
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import create_agent 
from langchain_core.tools import tool 
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

# LangGraph (pour le Checkpoint/M√©moire uniquement)
from langgraph.checkpoint.memory import InMemorySaver 

# --- VARIABLES GLOBALES ---
thread_id = "user_123_session"
PROJECT_NAME = "RAG_Agent_LTM_Tracing" 
# Les variables globales LLM/Agent sont maintenant initialis√©es dans cl.on_chat_start
# Nous conservons la session Phoenix au niveau global pour l'√©valuation de fin
global session 
session = None

# =====================================================
# 0Ô∏è‚É£ INITIALISATION PHOENIX (Tracing)
# =====================================================

# Nous laissons l'initialisation Phoenix se faire au d√©marrage du script, 
# car elle doit se faire avant que l'instrumentation LangChain ne soit utilis√©e.
print(f"üöÄ D√©marrage de Phoenix pour le projet : **{PROJECT_NAME}**...")
try:
    # 1. Lance l'application Phoenix
    session = ph.launch_app()
    print(f"üìà Phoenix UI d√©marr√© ! Consultez : {session.url}")
    
    # 2. Configure le Phoenix tracer (votre configuration valid√©e)
    tracer_provider = register(
        project_name=PROJECT_NAME,
        endpoint="http://localhost:6006/v1/traces"
    )

    # 3. Instrumente LangChain avec le tracer provider sp√©cifique
    LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
    print("‚úÖ Instrumentation LangChain/Phoenix r√©ussie.")

    # 4. R√©cup√®re le tracer (non strictement n√©cessaire ici, car l'agent est auto-instrument√©)
    tracer = tracer_provider.get_tracer(__name__)

except Exception as e:
    print(f"‚ùå Erreur critique lors du d√©marrage/instrumentation de Phoenix. Le tracing sera d√©sactiv√©: {e}")
    session = None

print("------------------------------------------------------------------")

# =====================================================
# 1Ô∏è‚É£ Configuration de base et Mod√®le (Pass√©e √† cl.on_chat_start)
# =====================================================

# Les fonctions utilitaires (save_to_long_term_memory) peuvent rester en dehors
def save_to_long_term_memory(thread_id: str, user_query: str, ai_response: str, vectordb_history_instance):
    """Enregistre la paire de messages dans la base Chroma pour la LTM."""
    content = f"Utilisateur ({thread_id}): {user_query}\nIA ({thread_id}): {ai_response}"
    vectordb_history_instance.add_texts([content], metadata={"thread_id": thread_id})


# =====================================================
# 2Ô∏è‚É£ D√©finition des Outils (Tool)
# =====================================================
# Les outils doivent √™tre d√©finis globalement ou transmis
load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("‚ö†Ô∏è GOOGLE_API_KEY manquante dans le fichier .env")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

vectordb_rag_global = Chroma(embedding_function=embeddings, persist_directory="chroma_db")
vectordb_history_global = Chroma(embedding_function=embeddings, persist_directory="chroma_history")

@tool
def retrieve_context(query: str) -> str:
    """
    R√©cup√®re les documents les plus pertinents de la base de donn√©es 
    documentaire (RAG) pour r√©pondre √† une question factuelle.
    """
    results = vectordb_rag_global.similarity_search(query, k=3) 
    if not results:
        return "‚ùå Aucun document RAG pertinent trouv√©."
    
    rag_context = ""
    source_documents = [] # Liste pour stocker les documents bruts pour l'affichage
    source_names_list = [] # Liste pour forcer le LLM √† citer les sources

    for i, r in enumerate(results):
        source_path = r.metadata.get('source', f'Source RAG {i+1}')
        citation_name = f"source_{i+1}" # Nom Chainlit : source_1, source_2... (Nom simple pour la citation LLM)
        content = r.page_content
        
        # üÜï ADAPTATION : Cr√©er le nom d'affichage convivial
        try:
            # Tente de rendre le nom plus lisible (ex: enlever le chemin et reformater la page)
            base_name = os.path.basename(source_path)
            if ':page_' in base_name:
                # Ex: 'document.pdf:page_10' devient 'document.pdf - Page 10'
                display_name_friendly = base_name.replace(':page_', ' - Page ')
            else:
                # Si le format est juste le nom du fichier, utilise le nom et l'index du chunk
                display_name_friendly = f"{base_name} (Chunk {i+1})"
        except:
            display_name_friendly = f"Source {i+1} (D√©tails)"
        
        # 1. Stocke les documents bruts (pour l'affichage futur)
        source_documents.append({
            "content": content,
            "source": source_path,
            "name": citation_name, # Nom simple pour le lien cliquable
            "display_name": display_name_friendly # üëà AJOUT DE LA CL√â DISPLAY_NAME
        })
        
        # 2. Ajoute le nom de la source √† la liste des citations
        source_names_list.append(citation_name)
        
        # 3. Construction du contexte RAG pour l'Agent
        rag_context += f"[DOCUMENT RAG {i+1} - CITATION: {citation_name}]: {content}\n---\n"
    
    # 4. ‚ö†Ô∏è STOCKAGE des DOCUMENTS bruts dans la session utilisateur
    cl.user_session.set("documents_to_display", source_documents) 
    
    # 5. Ajout d'une instruction forte pour forcer la citation des sources
    citation_instruction = f"\n\n**INSTRUCTION LLM:** Lorsque tu r√©ponds √† la question, utilise les sources ci-dessus et ajoute OBLIGATOIREMENT √† la fin de ta r√©ponse la liste des sources cit√©es sous la forme : **Sources: {', '.join(source_names_list)}**."

    return rag_context + citation_instruction
@tool
def retrieve_history(query: str) -> str:
    """
    R√©cup√®re des fragments de conversations pass√©es pertinentes de la m√©moire 
    historique (chroma_history) pour le contexte √† long terme.
    """
    results = vectordb_history_global.similarity_search(query, k=2) 
    
    if not results:
        return "‚ùå Aucun fragment de conversation pass√©e pertinent trouv√©."
    
    history_context = "\n\n".join(
        [f"üí¨ Fragment de conversation: {r.page_content[:200]}..." for r in results]
    )
    return history_context

tools = [retrieve_context, retrieve_history]


# =====================================================
# 3Ô∏è‚É£ Cr√©ation de l'Agent (cl.on_chat_start)
# =====================================================

# Chainlit d√©marre l'agent ici et stocke les instances dans le "user session"
@cl.on_chat_start
async def start():
    
    # Mod√®le LLM
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite", 
        temperature=0.3,
    )

    
    # üÜï Configuration du Checkpointer LangGraph avec chemin absolu
    # Notez le changement de syntaxe de la cha√Æne de connexion pour le chemin absolu
    checkpointer = InMemorySaver()

    system_prompt_text = (
        "Tu es un assistant RAG professionnel avec une m√©moire √† long terme. "
        "Tu as acc√®s √† deux outils : "
        "'retrieve_context' pour les faits documentaires g√©n√©raux, "
        "'retrieve_history' pour te rappeler des discussions ant√©rieures"
        " Utilise l'outil le plus appropri√© pour chaque requ√™te. R√©ponds poliment."
    )
    
    # Cr√©ation de l'Agent
    agent_instance = create_agent(
        llm, 
        tools, 
        system_prompt=system_prompt_text, 
        checkpointer=checkpointer, 
    )

    # Sauvegarde l'agent et la DB d'historique dans la session Chainlit
    cl.user_session.set("agent", agent_instance)
    cl.user_session.set("llm", llm) 
    cl.user_session.set("system_prompt_text", system_prompt_text) 
    cl.user_session.set("checkpointer", checkpointer) 
    
    cl.user_session.set("vectordb_history", vectordb_history_global)
    cl.user_session.set("thread_id", thread_id)
    cl.user_session.set("documents_to_display", [])

    await cl.Message(content="Bonjour ! Je suis l'Agent RAG avec m√©moire. Posez-moi votre premi√®re question.").send()


# =====================================================
# 4Ô∏è‚É£ Boucle de r√©ponse (cl.on_message)
# =====================================================

@cl.on_message
async def main(message: cl.Message):
    # R√©cup√©ration des objets depuis la session utilisateur
    vectordb_history_instance = cl.user_session.get("vectordb_history")
    thread_id_instance = cl.user_session.get("thread_id")
    agent_instance = cl.user_session.get("agent")
    query = message.content
    
    # -----------------------------------------------------------
    # √âTAPE 1 : EX√âCUTION DE L'AGENT AVEC STREAMING
    # -----------------------------------------------------------
    
    checkpointer_config = {
        "configurable": {
            "thread_id": thread_id_instance 
        }
    }
    
    initial_messages = [HumanMessage(content=query)]
    
    # Nous envoyons un message temporaire pour signaler l'activit√© (optionnel)
    final_response_text = ""
    msg = cl.Message(content="ü§ñ R√©flexion en cours...")
    await msg.send()

    try:
        # Lancement du stream de l'agent LangGraph
        async for token, metadata in agent_instance.astream( # üëà Agent stock√©
            {"messages": initial_messages}, 
            config=checkpointer_config,
            stream_mode="messages",
        ):
            message_token = token
            
            if isinstance(message_token, AIMessage) and message_token.content:
                await msg.stream_token(message_token.content)
                final_response_text += message_token.content
            
        # Finalisation de l'affichage Chainlit
        await msg.update() # üëà Le message du LLM est maintenant complet

        # -----------------------------------------------------------
        # √âTAPE 2 : PR√âPARATION ET ENVOI DU MESSAGE FINAL AVEC SOURCES
        # -----------------------------------------------------------

        # 1. R√©cup√®re les documents bruts stock√©s par l'outil
        source_documents = cl.user_session.get("documents_to_display")
        text_elements = []

        if source_documents:
            # Cr√©e les cl.Text √©l√©ments (utilisant la nouvelle structure du doc)
            for doc in source_documents:

                # R√©cup√®re le nom convivial s'il existe, sinon utilise l'ancien format
                display_name_text = doc.get('display_name', f"{doc['name']} ({doc['source']})")

                text_elements.append(
                    cl.Text(
                        content=doc['content'], 
                        name=doc['name'], # üëà Nom simple pour le lien (e.g., source_1)
                        display="side",
                        display_name=display_name_text # üëà Nom du document/page
                    )
                )
            
            # 2. Attache les √©l√©ments au message final
            msg.elements = text_elements
            await msg.update() # üëà Mise √† jour finale pour afficher les sources

            # 3. Vider la liste apr√®s utilisation
            cl.user_session.set("documents_to_display", []) 

        # 4. Sauvegarde dans la LTM
        save_to_long_term_memory(thread_id_instance, query, final_response_text, vectordb_history_instance)
        print(f"üíæ Conversation enregistr√©e dans 'chroma_history' (LTM) pour le thread {thread_id_instance}.")

    except Exception as e:
        error_message = (
            f"‚ùå Erreur critique LangGraph/Chainlit : {e}"
        )
        await cl.Message(content=error_message).send()
        print(error_message)