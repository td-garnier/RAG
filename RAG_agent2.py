import os
import sys 
from dotenv import load_dotenv

# üÜï --- IMPORTS CHAINLIT ---
import chainlit as cl
# --- FIN IMPORTS CHAINLIT ---

# üÜï --- IMPORTS PHOENIX & INSTRUMENTATION ---
import phoenix as ph
from phoenix.otel import register
from openinference.instrumentation.langchain import LangChainInstrumentor 
from opentelemetry import trace
from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
    OpenAIModel,
    run_evals,
)
import pandas as pd 
# --- FIN IMPORTS PHOENIX ---

# LangChain/LLM/RAG/Agents
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import create_agent 
from langchain_core.tools import tool 
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

# LangGraph (pour le Checkpoint/M√©moire uniquement)
from langgraph.checkpoint.memory import InMemorySaver 
from RAG_core import retrieve_context_core, vectordb_rag_global, vectordb_history_global

# --- VARIABLES GLOBALES ---
thread_id = "user_123_session"
PROJECT_NAME = "RAG_Agent_LTM_Tracing" 
# Les variables globales LLM/Agent sont maintenant initialis√©es dans cl.on_chat_start
# Nous conservons la session Phoenix au niveau global pour l'√©valuation de fin
global session 
session = None

# =====================================================
# 0Ô∏è‚É£ INITIALISATION PHOENIX (Tracing)
# =====================================================

# Nous laissons l'initialisation Phoenix se faire au d√©marrage du script, 
# car elle doit se faire avant que l'instrumentation LangChain ne soit utilis√©e.
print(f"üöÄ D√©marrage de Phoenix pour le projet : **{PROJECT_NAME}**...")
try:
    # 1. Lance l'application Phoenix
    session = ph.launch_app()
    print(f"üìà Phoenix UI d√©marr√© ! Consultez : {session.url}")
    
    # 2. Configure le Phoenix tracer (votre configuration valid√©e)
    tracer_provider = register(
        project_name=PROJECT_NAME,
        endpoint="http://localhost:6006/v1/traces"
    )

    # 3. Instrumente LangChain avec le tracer provider sp√©cifique
    LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
    print("‚úÖ Instrumentation LangChain/Phoenix r√©ussie.")

    # 4. R√©cup√®re le tracer (non strictement n√©cessaire ici, car l'agent est auto-instrument√©)
    tracer = tracer_provider.get_tracer(__name__)

except Exception as e:
    print(f"‚ùå Erreur critique lors du d√©marrage/instrumentation de Phoenix. Le tracing sera d√©sactiv√©: {e}")
    session = None

print("------------------------------------------------------------------")

# =====================================================
# 1Ô∏è‚É£ Configuration de base et Mod√®le
# =====================================================

# fonctions utilitaires pour sauvegarder dans la LTM
def save_to_long_term_memory(thread_id: str, user_query: str, ai_response: str, vectordb_history_instance):
    """Enregistre la paire de messages dans la base Chroma pour la LTM."""
    content = f"Utilisateur ({thread_id}): {user_query}\nIA ({thread_id}): {ai_response}"
    vectordb_history_instance.add_texts([content], metadata={"thread_id": thread_id})


# =====================================================
# 2Ô∏è‚É£ D√©finition des Outils (Tool)
# =====================================================
# Les outils doivent √™tre d√©finis globalement ou transmis
load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("‚ö†Ô∏è GOOGLE_API_KEY manquante dans le fichier .env")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

vectordb_rag_global = Chroma(embedding_function=embeddings, persist_directory="chroma_db")
vectordb_history_global = Chroma(embedding_function=embeddings, persist_directory="chroma_history")

@tool
def retrieve_context(query: str) -> str:
    """
    R√©cup√®re les documents les plus pertinents de la base de donn√©es 
    documentaire (RAG) pour r√©pondre √† une question factuelle.
    """
    # üéØ 1. Appel de la logique RAG CORE pour obtenir le contexte format√©
    context_for_llm = retrieve_context_core(query) 

    # 2. Logiciel d'affichage Chainlit : R√©cup√©rer les documents bruts pour cl.Pdf
    
    # Utilisez la DB globale import√©e de RAG_core
    retriever_rag = vectordb_rag_global.as_retriever(search_kwargs={"k": 4}) 
    source_documents = retriever_rag.invoke(query) 

    documents_for_display = []
    PDF_FOLDER = "./documents" 

    for doc in source_documents:
        # Cr√©er les objets n√©cessaires pour cl.user_session.set()
        source_name_only = doc.metadata.get('source', 'Inconnu')
        pdf_path = os.path.join(PDF_FOLDER, source_name_only)
        page_number_for_display = doc.metadata.get('page_label', 1)
        citation_name = f"source_{len(documents_for_display) + 1}"
        display_name_friendly = f"{source_name_only} (Page {page_number_for_display})"
        
        documents_for_display.append({
            "content": doc.page_content,
            "source": source_name_only, 
            "name": citation_name,
            "display_name": display_name_friendly,
            "path": pdf_path,
            "page": page_number_for_display
        })

    # 3. Mise √† jour de la session Chainlit
    cl.user_session.set("documents_to_display", documents_for_display)

    return context_for_llm # Renvoyer le contexte format√© par la fonction CORE

@tool
def retrieve_history(query: str) -> str:
    """
    R√©cup√®re des fragments de conversations pass√©es pertinentes de la m√©moire 
    historique (chroma_history) pour le contexte √† long terme.
    """
    results = vectordb_history_global.similarity_search(query, k=2) 
    
    if not results:
        return "‚ùå Aucun fragment de conversation pass√©e pertinent trouv√©."
    
    history_context = "\n\n".join(
        [f"üí¨ Fragment de conversation: {r.page_content[:200]}..." for r in results]
    )
    return history_context

tools = [retrieve_context, retrieve_history]


# =====================================================
# 3Ô∏è‚É£ Cr√©ation de l'Agent (cl.on_chat_start)
# =====================================================

# Chainlit d√©marre l'agent ici et stocke les instances dans le "user session"
@cl.on_chat_start
async def start():
    
    # Mod√®le LLM
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite", 
        temperature=0,
    )

    
    # üÜï Configuration du Checkpointer LangGraph avec chemin absolu
    # Notez le changement de syntaxe de la cha√Æne de connexion pour le chemin absolu
    checkpointer = InMemorySaver()

    system_prompt_text = (
        "Tu es un assistant RAG professionnel avec une m√©moire √† long terme. "
        "Tu as acc√®s √† deux outils : "
        "'retrieve_context' pour les faits documentaires g√©n√©raux, "
        "'retrieve_history' pour te rappeler des discussions ant√©rieures"
        " Utilise l'outil le plus appropri√© pour chaque requ√™te. R√©ponds poliment."
    )
    
    # Cr√©ation de l'Agent
    agent_instance = create_agent(
        llm, 
        tools, 
        system_prompt=system_prompt_text, 
        checkpointer=checkpointer, 
    )

    # Sauvegarde l'agent et la DB d'historique dans la session Chainlit
    cl.user_session.set("agent", agent_instance)
    cl.user_session.set("llm", llm) 
    cl.user_session.set("system_prompt_text", system_prompt_text) 
    cl.user_session.set("checkpointer", checkpointer) 
    
    cl.user_session.set("vectordb_history", vectordb_history_global)
    cl.user_session.set("thread_id", thread_id)
    cl.user_session.set("documents_to_display", [])

    await cl.Message(content="Bonjour ! Je suis l'Agent RAG avec m√©moire. Posez-moi votre premi√®re question.").send()


# =====================================================
# 4Ô∏è‚É£ Boucle de r√©ponse (cl.on_message)
# =====================================================

@cl.on_message
async def main(message: cl.Message):
    # R√©cup√©ration des objets depuis la session utilisateur
    vectordb_history_instance = cl.user_session.get("vectordb_history")
    thread_id_instance = cl.user_session.get("thread_id")
    agent_instance = cl.user_session.get("agent")
    query = message.content
    
    # -----------------------------------------------------------
    # √âTAPE 1 : EX√âCUTION DE L'AGENT AVEC STREAMING
    # -----------------------------------------------------------
    
    checkpointer_config = {
        "configurable": {
            "thread_id": thread_id_instance 
        }
    }
    
    initial_messages = [HumanMessage(content=query)]
    
    # Nous envoyons un message temporaire pour signaler l'activit√© (optionnel)
    final_response_text = ""
    msg = cl.Message(content="ü§ñ R√©flexion en cours...")
    await msg.send()

    try:
        # Lancement du stream de l'agent LangGraph
        msg.content = ""
        await msg.update()

        async for token, metadata in agent_instance.astream( # üëà Agent stock√©
            {"messages": initial_messages}, 
            config=checkpointer_config,
            stream_mode="messages",
        ):
            message_token = token
            
            if isinstance(message_token, AIMessage) and message_token.content:
                await msg.stream_token(message_token.content)
                final_response_text += message_token.content
            
        # Finalisation de l'affichage Chainlit
        await msg.update() # üëà Le message du LLM est maintenant complet

        # -----------------------------------------------------------
        # √âTAPE 2 : PR√âPARATION ET ENVOI DU MESSAGE FINAL AVEC SOURCES
        # -----------------------------------------------------------

        # 1. R√©cup√®re les documents bruts stock√©s par l'outil
        source_documents = cl.user_session.get("documents_to_display")
        text_elements = []
        elements_to_attach = []

        if source_documents:
            for doc in source_documents:
                # V√©rifie si le chemin d'acc√®s au PDF est valide et existe
                if os.path.exists(doc.get('path')):
                    
                    elements_to_attach.append(
                        # üÜï Utilisation de cl.Pdf
                        cl.Pdf(
                            name=doc['name'], 
                            display="side", # Affichage dans le panneau lat√©ral
                            path=doc['path'], # Chemin local du fichier PDF
                            page=doc.get('page', 1), # Page √† laquelle ouvrir le PDF (par d√©faut 1)
                            display_name=doc.get('display_name', doc['name']) # Nom convivial
                        )
                    )
                else:
                    # Si le PDF n'est pas trouv√© (erreur dans le chemin), 
                    # on affiche au moins le chunk textuel comme secours
                    elements_to_attach.append(
                        cl.Text(
                            content=doc['content'], 
                            name=doc['name'],
                            display="side",
                            display_name=doc.get('display_name', doc['name'])
                        )
                    )
            
            # 2. Attache les √©l√©ments (maintenant des cl.Pdf) au message final
            msg.elements = elements_to_attach
            await msg.update()

            # 3. Vider la liste apr√®s utilisation
            cl.user_session.set("documents_to_display", []) 

        # 4. Sauvegarde dans la LTM
        save_to_long_term_memory(thread_id_instance, query, final_response_text, vectordb_history_instance)
        print(f"üíæ Conversation enregistr√©e dans 'chroma_history' (LTM) pour le thread {thread_id_instance}.")

    except Exception as e:
        error_message = (
            f"‚ùå Erreur critique LangGraph/Chainlit : {e}"
        )
        await cl.Message(content=error_message).send()
        print(error_message)