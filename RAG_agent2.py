import os
import sys 
from dotenv import load_dotenv

# ğŸ†• --- IMPORTS PHOENIX & INSTRUMENTATION ---
import phoenix as ph
from phoenix.otel import register
from openinference.instrumentation.langchain import LangChainInstrumentor 
from opentelemetry import trace
from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
    OpenAIModel,
    run_evals
)
# --- FIN IMPORTS PHOENIX ---

# LangChain/LLM/RAG/Agents
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import create_agent 
from langchain_core.tools import tool 
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

# LangGraph (pour le Checkpoint/MÃ©moire uniquement)
from langgraph.checkpoint.memory import InMemorySaver 

# --- VARIABLES GLOBALES ---
thread_id = "user_123_session"
# ğŸ†• NOM DU PROJET POUR ORGANISER LES TRACES DANS PHOENIX
PROJECT_NAME = "RAG_Agent_LTM_Tracing" 
session = None
vectordb_rag = None
vectordb_history = None
llm = None
agent = None

# =====================================================
# 0ï¸âƒ£ INITIALISATION PHOENIX (Tracing)
# =====================================================
print(f"ğŸš€ DÃ©marrage de Phoenix pour le projet : **{PROJECT_NAME}**...")
try:
    # 1. Lance l'application Phoenix et spÃ©cifie le nom du projet
    session = ph.launch_app()
    print(f"ğŸ“ˆ Phoenix UI dÃ©marrÃ© ! Consultez : {session.url}")
    
    # configure the Phoenix tracer
    tracer_provider = register(
        project_name=PROJECT_NAME, # Default is 'default'
        endpoint="http://localhost:6006/v1/traces"
        )

    # 2. Instrumente LangChain pour l'envoi automatique des traces
    LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
    print("âœ… Instrumentation LangChain/Phoenix rÃ©ussie.")

    tracer = tracer_provider.get_tracer(__name__)

except Exception as e:
    print(f"âŒ Erreur critique lors du dÃ©marrage/instrumentation de Phoenix. Le tracing sera dÃ©sactivÃ©: {e}")
    session = None

print("------------------------------------------------------------------")

# =====================================================
# 1ï¸âƒ£ Configuration de base et ModÃ¨le (InchangÃ©)
# =====================================================
load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("âš ï¸ GOOGLE_API_KEY manquante dans le fichier .env")

# Configuration des Embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# 1. Base Chroma pour les Documents (RAG)
vectordb_rag = Chroma(
    embedding_function=embeddings,
    persist_directory="chroma_db",
)

# 2. Base Chroma pour l'Historique (MÃ©moire Ã  Long Terme / LTM)
vectordb_history = Chroma(
    embedding_function=embeddings,
    persist_directory="chroma_history",
)

# ModÃ¨le LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite", 
    temperature=0.3,
)

# Configuration du Checkpointer LangGraph
checkpointer = InMemorySaver() 

# =====================================================
# 2ï¸âƒ£ DÃ©finition des Outils (Tool) (InchangÃ©)
# =====================================================

@tool
def retrieve_context(query: str) -> str:
    """
    RÃ©cupÃ¨re les documents les plus pertinents de la base de donnÃ©es 
    documentaire (RAG) pour rÃ©pondre Ã  une question factuelle.
    """
    results = vectordb_rag.similarity_search(query, k=3) 
    if not results:
        return "âŒ Aucun document RAG pertinent trouvÃ©."
    
    rag_context = "\n\n".join(
        [f"ğŸ“„ Source: {r.metadata.get('source', 'inconnu')}:\n{r.page_content[:200]}..." for r in results]
    )
    return rag_context

@tool
def retrieve_history(query: str) -> str:
    """
    RÃ©cupÃ¨re des fragments de conversations passÃ©es pertinentes de la mÃ©moire 
    historique (chroma_history) pour le contexte Ã  long terme.
    """
    results = vectordb_history.similarity_search(query, k=2) 
    
    if not results:
        return "âŒ Aucun fragment de conversation passÃ©e pertinent trouvÃ©."
    
    history_context = "\n\n".join(
        [f"ğŸ’¬ Fragment de conversation: {r.page_content[:200]}..." for r in results]
    )
    return history_context

tools = [retrieve_context, retrieve_history]

# =====================================================
# 3ï¸âƒ£ CrÃ©ation et Utilitaires de l'Agent (InchangÃ©)
# =====================================================

system_prompt_text = (
    "Tu es un assistant RAG professionnel avec une mÃ©moire Ã  long terme. "
    "Tu as accÃ¨s Ã  deux outils : 'retrieve_context' pour les faits documentaires et "
    "rÃ©pondre Ã  une question factuelle et 'retrieve_history' pour te rappeler des discussions antÃ©rieures."
    "Utilise l'outil le plus appropriÃ© pour chaque requÃªte. RÃ©ponds poliment."
)

agent = create_agent(
    llm, 
    tools, 
    system_prompt=system_prompt_text, 
    checkpointer=checkpointer, 
)

def save_to_long_term_memory(thread_id: str, user_query: str, ai_response: str):
    """Enregistre la paire de messages dans la base Chroma pour la LTM."""
    global vectordb_history
    content = f"Utilisateur ({thread_id}): {user_query}\nIA ({thread_id}): {ai_response}"
    vectordb_history.add_texts([content], metadata={"thread_id": thread_id})


# =====================================================
# 4ï¸âƒ£ Boucle interactive de l'Agent avec Streaming (InchangÃ©)
# =====================================================
print(f"ğŸ¤– Agent LangChain (create_agent) + Streaming + MÃ©moire LTM/STM prÃªt ! (ID : {thread_id})")
print(f"âœ… Outils RAG/Historique. La rÃ©ponse sera affichÃ©e en temps rÃ©el.\n")

while True:
    query = input("ğŸ§  Votre question : ").strip()

    if query.lower() in ["exit", "quit"]:
        print("\nğŸ‘‹ Fin de la session. Ã€ bientÃ´t !")
        break

    initial_messages = [HumanMessage(content=query)]
    checkpointer_config = {
        "configurable": {
            "thread_id": thread_id 
        }
    }

    print("ğŸ’­ L'Agent rÃ©flÃ©chit (Streaming)...")
    print("\nğŸ¤– RÃ©ponse :\n")
    final_response_text = ""
    
    try:
        for token, metadata in agent.stream(
            {"messages": initial_messages},
            config=checkpointer_config,
            stream_mode="messages",
        ):
            message = token
            
            if isinstance(message, AIMessage) and message.content and not message.tool_calls:
                print(message.content, end="", flush=True)
                final_response_text += message.content
            
        print("\n")

        save_to_long_term_memory(thread_id, query, final_response_text)
        print("ğŸ’¾ Le tour de conversation a Ã©tÃ© enregistrÃ© dans 'chroma_history'.\n")

    except Exception as e:
        print(f"\nâŒ Erreur pendant la gÃ©nÃ©ration : {e}")