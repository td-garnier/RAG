import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import ChatGoogleGenerativeAI

# =====================================================
# Configuration
# =====================================================
pdf_folder = "./documents"
persist_dir = "chroma_db"

load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("‚ö†Ô∏è GOOGLE_API_KEY manquante dans le fichier .env")

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# =====================================================
# Charger la base Chroma existante (si elle existe)
# =====================================================
vectordb = None
if os.path.exists(persist_dir):
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    print("üì¶ Base Chroma existante charg√©e.")
else:
    print("üÜï Aucune base Chroma trouv√©e ‚Äî une nouvelle sera cr√©√©e.")

# =====================================================
# Identifier les fichiers d√©j√† index√©s
# =====================================================
indexed_files = set()
if vectordb is not None:
    try:
        collection = vectordb._collection.get(include=["metadatas"])
        if collection and "metadatas" in collection:
            for meta in collection["metadatas"]:
                if meta and "source" in meta:
                    indexed_files.add(meta["source"])
        print(f"üîç {len(indexed_files)} fichiers d√©j√† index√©s : {indexed_files}")
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible de lire les m√©tadonn√©es : {e}")

# =====================================================
# Pr√©paration du LLM pour la g√©n√©ration de tags
# =====================================================
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0,
    max_tokens=200,
    timeout=None,
    max_retries=2
)

# =====================================================
# Parcours des nouveaux PDF uniquement
# =====================================================
all_docs = []
for file_name in os.listdir(pdf_folder):
    if not file_name.lower().endswith(".pdf"):
        continue
    if file_name in indexed_files:
        print(f"‚è≠Ô∏è Fichier d√©j√† index√© : {file_name}")
        continue

    file_path = os.path.join(pdf_folder, file_name)
    print(f"üìÑ Nouveau fichier d√©tect√© : {file_name}")

    try:
        # Charger le texte uniquement via PyPDF
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        content_excerpt = " ".join([d.page_content[:2000] for d in docs])

        # G√©n√©ration des tags
        messages = [
            ("system", "Tu lis un document et g√©n√®res 2 ou 3 mots-cl√©s r√©sumant le sujet principal."),
            ("human", f"Voici le texte : {content_excerpt}\nDonne seulement un ou deux mots-cl√©s s√©par√©s par des virgules.")
        ]
        ai_msg = llm.invoke(messages)
        tag = ai_msg.content.strip()
        print(f"üß© Tags g√©n√©r√©s pour {file_name} : {tag}")

        # Ajouter les m√©tadonn√©es
        for doc in docs:
            doc.metadata["source"] = file_name
            doc.metadata["tag"] = tag

        all_docs.extend(docs)

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du chargement de {file_name}: {e}")

# =====================================================
# Si aucun nouveau document, on arr√™te
# =====================================================
if not all_docs:
    print("‚úÖ Aucun nouveau document √† ajouter.")
    exit()

# =====================================================
# Split et ajout √† la base
# =====================================================
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)
all_splits = splitter.split_documents(all_docs)

print(f"üß© {len(all_splits)} nouveaux chunks √† indexer.")

if vectordb is None:
    vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=persist_dir)
    print("‚úÖ Nouvelle base vectorielle cr√©√©e.")
else:
    vectordb.add_documents(all_splits)
    print("‚úÖ Nouveaux documents ajout√©s √† la base existante.")

vectordb.persist()
print("üíæ Base vectorielle sauvegard√©e avec succ√®s.")
