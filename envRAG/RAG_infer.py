import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI

# =====================================================
# 1Ô∏è‚É£  Chargement des variables d'environnement
# =====================================================
load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("‚ö†Ô∏è GOOGLE_API_KEY manquante dans le fichier .env")

# =====================================================
# 2Ô∏è‚É£  Chargement du mod√®le d'embeddings et de la base Chroma
# =====================================================
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
vectordb = Chroma(persist_directory="chroma_db", embedding_function=embeddings)

# =====================================================
# 3Ô∏è‚É£  Initialisation du mod√®le LLM Gemini
# =====================================================
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0.3,
    max_tokens=700,
    timeout=None,
    max_retries=2
)

# =====================================================
# 4Ô∏è‚É£  Boucle interactive
# =====================================================
print("ü§ñ Assistant RAG pr√™t ! (tape 'exit' ou 'quit' pour arr√™ter)\n")

while True:
    query = input("üß† Votre question : ").strip()
    if query.lower() in ["exit", "quit"]:
        print("\nüëã Fin de la session. √Ä bient√¥t !")
        break

    # Recherche vectorielle
    results = vectordb.similarity_search(query, k=10)

    # Extraire les tags des r√©sultats
    tags = [r.metadata.get("tag", "inconnu") for r in results]
    unique_tags = list(set(tags))

    print(f"\nüîç {len(results)} extraits trouv√©s ‚Äî tags d√©tect√©s : {', '.join(unique_tags)}\n")

    # Concat√©ner les textes des documents trouv√©s
    if not results:
        print("‚ùå Aucun r√©sultat pertinent trouv√© dans la base.")
        continue

    rag_context = "\n\n".join(
        [f"Extrait {i+1} ({r.metadata.get('source', 'inconnu')}):\n{r.page_content}" for i, r in enumerate(results)]
    )

    # Pr√©parer le prompt pour Gemini
    messages = [
        (
            "system",
            "Tu es un assistant sp√©cialis√© en documentation technique. "
            "R√©ponds uniquement √† partir des extraits fournis ci-dessous, sans inventer d'informations."
        ),
        (
            "human",
            f"Voici les extraits trouv√©s par le RAG :\n{rag_context}\n\nQuestion : {query}"
        ),
    ]

    # G√©n√©ration de la r√©ponse
    try:
        ai_msg = llm.invoke(messages)
        print("\nü§ñ R√©ponse :\n")
        print(ai_msg.content.strip(), "\n")

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration de la r√©ponse : {e}")
