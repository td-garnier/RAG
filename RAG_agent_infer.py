import os
from typing import Annotated, TypedDict
from dotenv import load_dotenv

# LangChain/LangGraph/LLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages, AnyMessage
from langchain_core.messages import SystemMessage, HumanMessage
from pydantic import BaseModel, Field

# NOUVEAUX IMPORTS pour l'architecture Agent/Tool Calling
from langchain_core.tools import tool 
from langgraph.prebuilt import ToolNode, tools_condition 


# =====================================================
# 1ï¸âƒ£ DÃ©finition de lâ€™Ã©tat du graphe (AgentState)
# =====================================================
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    # 'rag_context' est conservÃ© pour la clartÃ© (bien qu'il soit maintenant gÃ©rÃ© par l'outil)
    rag_context: str
    history_context: str
    thread_id: str
    user_query: str


# =====================================================
# 2ï¸âƒ£ Configuration de base (InchangÃ©e)
# =====================================================
load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("âš ï¸ GOOGLE_API_KEY manquante dans le fichier .env")

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

history_db = Chroma(
    persist_directory="chroma_history",
    embedding_function=embeddings,
)

vectordb = Chroma(
    persist_directory="chroma_db",
    embedding_function=embeddings,
)

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0.3,
    max_tokens=700,
    timeout=None,
    max_retries=2,
)


# =====================================================
# 3ï¸âƒ£ DÃ©finition des Outils et NÅ“uds
# =====================================================

# DÃ©finition de l'outil RAG (InchangÃ©)
@tool
def rag_search_tool(query: str) -> str:
    """Utilisez cet outil pour trouver des informations FACTUELLES dans la documentation technique (aspirateur, airfryer, etc.)."""
    
    results = vectordb.similarity_search(query, k=5)
    
    if not results:
        return "âŒ Aucun rÃ©sultat pertinent trouvÃ© dans la base documentaire."
    
    rag_context = "\n\n".join(
        [f"ğŸ“„ Source: {r.metadata.get('source', 'inconnu')}:\n{r.page_content}" for r in results]
    )
    return rag_context

# Initialisation de l'Agent LLM (Seul le LLM est liÃ© aux outils)
tools = [rag_search_tool] 
agent_llm = llm.bind_tools(tools=tools) 


# NÅ“ud 1 : RÃ©cupÃ©ration de l'Historique (InchangÃ©)
def history_retriever_node(state: AgentState):
    """RÃ©cupÃ¨re les 5 derniers Ã©changes du thread actuel dans la base de mÃ©moire (Chroma)."""
    thread_id = state["thread_id"]
    
    thread_history = []
    try:
        all_history = history_db.get(include=["metadatas", "documents"])
        for doc, meta in zip(all_history["documents"], all_history["metadatas"]):
            content = doc.get("page_content", doc) if isinstance(doc, dict) else str(doc)
            
            if meta.get("thread_id") == thread_id:
                thread_history.append(content)
    except Exception as e:
        print(f"âš ï¸ Erreur lors de la rÃ©cupÃ©ration de lâ€™historique : {e}")
        
    history_context = "\n".join(thread_history[-5:])
    
    return {"history_context": history_context}


# NÅ“ud 2 : Agent Node (Le Cerveau - InchangÃ©)
def agent_node(state: AgentState):
    """ReÃ§oit l'historique des messages et dÃ©cide d'appeler un outil ou de rÃ©pondre."""
    messages = state["messages"]
    
    print("ğŸ§  Agent: DÃ©cision...")
    
    # Le LLM, liÃ© aux outils, dÃ©cide s'il doit utiliser un outil ou rÃ©pondre directement.
    response = agent_llm.invoke(messages)
    
    return {"messages": [response]} 


# NOUVEAU NÅ“ud 3 : Sauvegarde (answer_node simplifiÃ© - InchangÃ©)
def answer_node(state: AgentState):
    """GÃ¨re la sauvegarde de l'Ã©change final dans l'historique."""
    
    thread_id = state["thread_id"]
    user_query = state["user_query"]
    
    response_message = state["messages"][-1]
    text_response = getattr(response_message, "content", "")

    print(f"âœ… SynthÃ¨se: Sauvegarde de l'Ã©change.")
    
    try:
        history_db.add_texts(
            texts=[f"USER: {user_query}\nAI: {text_response}"],
            metadatas=[{"thread_id": thread_id}],
        )
    except Exception as e:
        print(f"âš ï¸ Erreur lors de la sauvegarde de lâ€™historique : {e}")

    return {"messages": [response_message]} 


# =====================================================
# 4ï¸âƒ£ CrÃ©ation du Graphe LangGraph (Utilisation de ToolNode)
# =====================================================
checkpointer = InMemorySaver()
builder = StateGraph(AgentState)

# Ajouter les nÅ“uds
builder.add_node("history_retriever_node", history_retriever_node)
builder.add_node("agent_node", agent_node)
builder.add_node("tool_executor", ToolNode(tools)) 
builder.add_node("answer_node", answer_node)

# 1. DÃ©but -> RÃ©cupÃ©ration de l'historique
builder.add_edge(START, "history_retriever_node")

# 2. AprÃ¨s l'historique, l'Agent prend la premiÃ¨re dÃ©cision
builder.add_edge("history_retriever_node", "agent_node")

# 3. Boucle conditionnelle Agent â†” Outil
builder.add_conditional_edges(
    "agent_node",
    # tools_condition est une fonction utilitaire qui vÃ©rifie si l'Agent a demandÃ© un outil (tool_calls)
    tools_condition,
    {
        # Si 'tools' est retournÃ© (Agent demande un outil) -> ExÃ©cuter l'outil
        "tools": "tool_executor", 
        # Si 'END' est retournÃ© (Agent a gÃ©nÃ©rÃ© la rÃ©ponse finale) -> Fin/Sauvegarde
        "end": "answer_node",   
    },
)

# AprÃ¨s l'exÃ©cution de l'outil, on retourne Ã  l'Agent pour qu'il analyse le rÃ©sultat et rÃ©ponde
builder.add_edge("tool_executor", "agent_node")

# Fin
builder.add_edge("answer_node", END)

# Compilation du graphe
agent_tool_calling_graph = builder.compile(checkpointer=checkpointer)


# =====================================================
# 5ï¸âƒ£ Boucle interactive de l'Agent (InchangÃ©e)
# =====================================================
thread_id = "main_thread"
print("ğŸ¤– Agent Tool Calling + RAG + MÃ©moire + LangGraph prÃªt !")
print("ğŸ’¬ Tapez votre question, ou 'history' pour voir les 5 derniers Ã©changes.\n")

while True:
    query = input("ğŸ§  Votre question : ").strip()

    if query.lower() in ["exit", "quit"]:
        print("\nğŸ‘‹ Fin de la session. Ã€ bientÃ´t !")
        break
    
    if query.lower() == "history":
        try:
            all_history = history_db.get(include=["metadatas", "documents"])
            thread_history = []
            for doc, meta in zip(all_history["documents"], all_history["metadatas"]):
                content = doc.get("page_content", doc) if isinstance(doc, dict) else str(doc)
                if meta.get("thread_id") == thread_id:
                    thread_history.append(content)

            if not thread_history:
                print("ğŸ•³ï¸ Aucun historique trouvÃ© pour ce thread.")
                continue

            print("\nğŸ“œ 5 derniers Ã©changes :\n")
            for entry in thread_history[-5:]:
                print(entry)
                print("-" * 40)
        except Exception as e:
            print(f"âš ï¸ Erreur lors de la rÃ©cupÃ©ration de lâ€™historique : {e}")
        continue


    initial_state = {
        "messages": [HumanMessage(content=query)], 
        "thread_id": thread_id,
        "user_query": query,
        "rag_context": "", 
        "history_context": "" 
    }

    print("ğŸ’­ L'Agent rÃ©flÃ©chit (Boucle Tool Calling)...")
    
    checkpointer_config = {
        "configurable": {
            "thread_id": thread_id 
        }
    }
    
    try:
        output = agent_tool_calling_graph.invoke(initial_state, config=checkpointer_config)

        messages_out = output.get("messages", [])
        if not messages_out:
            print("âš ï¸ Aucune rÃ©ponse gÃ©nÃ©rÃ©e.")
            continue

        last_msg = messages_out[-1]
        response_text = getattr(last_msg, "content", None)

        if not response_text:
            print(f"âš ï¸ Contenu vide : {last_msg}")
        else:
            print("\nğŸ¤– RÃ©ponse :\n")
            print(response_text.strip(), "\n")

    except Exception as e:
        print(f"âŒ Erreur pendant la gÃ©nÃ©ration : {e}")