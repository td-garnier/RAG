import os
from typing import Annotated, TypedDict
from dotenv import load_dotenv

# LangChain/LangGraph/LLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages, AnyMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers.pydantic import PydanticOutputParser
from langchain_core.messages import SystemMessage, HumanMessage
from pydantic import BaseModel, Field


# =====================================================
# 1ï¸âƒ£ DÃ©finition de lâ€™Ã©tat du graphe (AgentState)
# =====================================================
class AgentState(TypedDict):
    # 'messages' est gÃ©rÃ© par LangGraph (add_messages)
    messages: Annotated[list[AnyMessage], add_messages]
    # 'rag_context' stocke le rÃ©sultat de la recherche vectorielle (si effectuÃ©e)
    rag_context: str
    # 'history_context' stocke l'historique de conversation rÃ©cupÃ©rÃ© de Chroma
    history_context: str
    # 'thread_id' est explicitement ajoutÃ© Ã  l'Ã©tat pour les nÅ“uds
    thread_id: str
    # 'user_query' est conservÃ© pour la sauvegarde finale
    user_query: str


# =====================================================
# 2ï¸âƒ£ Configuration de base
# =====================================================
load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("âš ï¸ GOOGLE_API_KEY manquante dans le fichier .env")

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# MÃ©moire conversationnelle persistante (Chroma)
history_db = Chroma(
    persist_directory="chroma_history",
    embedding_function=embeddings,
)

# Base documentaire RAG
vectordb = Chroma(
    persist_directory="chroma_db",
    embedding_function=embeddings,
)

# ModÃ¨le LLM Gemini
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0.3,
    max_tokens=700,
    timeout=None,
    max_retries=2,
)


# =====================================================
# 3ï¸âƒ£ DÃ©finition des NÅ“uds de l'Agent
# =====================================================

# NÅ“ud 1 : RÃ©cupÃ©ration de l'Historique (Toujours)
def history_retriever_node(state: AgentState):
    """RÃ©cupÃ¨re les 5 derniers Ã©changes du thread actuel dans la base de mÃ©moire (Chroma)."""
    thread_id = state["thread_id"]
    
    thread_history = []
    try:
        all_history = history_db.get(include=["metadatas", "documents"])
        for doc, meta in zip(all_history["documents"], all_history["metadatas"]):
            # Utilisation de .get("page_content") pour gÃ©rer les diffÃ©rents types de documents Chroma
            content = doc.get("page_content", doc) if isinstance(doc, dict) else str(doc)
            
            if meta.get("thread_id") == thread_id:
                thread_history.append(content)
    except Exception as e:
        print(f"âš ï¸ Erreur lors de la rÃ©cupÃ©ration de lâ€™historique : {e}")
        
    history_context = "\n".join(thread_history[-5:])
    
    # Stocker l'historique dans l'Ã©tat pour les nÅ“uds suivants
    return {"history_context": history_context}


# ğŸ†• NÅ“ud 2 : Router (DÃ©cision LLM)
class RouteDecision(BaseModel):
    action: str = Field(description="Must be 'use_rag' if an external search is required, or 'final_answer' otherwise.")

parser = PydanticOutputParser(pydantic_object=RouteDecision)

router_prompt = ChatPromptTemplate.from_messages([
    ("system", 
     "Tu es un routeur intelligent. Analyse la question de l'utilisateur en tenant compte de l'Historique. "
     "Si la rÃ©ponse est dÃ©jÃ  contenue dans l'Historique, choisis 'final_answer'. "
     "Si la question est nouvelle, factuelle, et nÃ©cessite une recherche documentaire, choisis 'use_rag'. "
     "Sinon, si elle est purement conversationnelle (salutations, transition), choisis 'final_answer'. "
     "RÃ©ponds uniquement avec un objet JSON qui correspond au schÃ©ma donnÃ©."
     "\n\n--- Historique de conversation ---\n{history}"), # â¬…ï¸ Ajout de l'historique
     
    ("human", "Question Ã  analyser: {question}"), 
])

router_chain = router_prompt | llm.with_structured_output(RouteDecision) | (lambda x: x.action)

def router_node(state: AgentState) -> str:
    """NÅ“ud de routage conditionnel."""
    last_message = state["messages"][-1].content 
    history_context = state["history_context"] # â¬…ï¸ RÃ©cupÃ©ration de l'historique
    
    print(f"ğŸ”„ Routeur: Analyse de la question...")
    
    # Appel de la chaÃ®ne de routage avec l'historique
    decision = router_chain.invoke({
        "question": last_message,
        "history": history_context # â¬…ï¸ Passage de la variable 'history'
    })
    
    if decision == "use_rag":
        print("ğŸ” Routeur: DÃ©cision = Recherche RAG")
        return "rag_search_node"
    
    print("ğŸ’¬ Routeur: DÃ©cision = RÃ©ponse directe (Historique suffisant)")
    return "answer_node"


# ğŸ†• NÅ“ud 3 : Recherche RAG (ExÃ©cution d'outil conditionnelle)
def rag_search_node(state: AgentState):
    """ExÃ©cute la recherche vectorielle RAG et met Ã  jour l'Ã©tat avec le contexte trouvÃ©."""
    last_message = state["messages"][-1].content
    
    results = vectordb.similarity_search(last_message, k=5)
    
    if not results:
        print("âŒ RAG: Aucun rÃ©sultat pertinent trouvÃ©.")
        rag_context = "âŒ Aucun rÃ©sultat pertinent trouvÃ© dans la base documentaire."
    else:
        print(f"âœ… RAG: {len(results)} documents trouvÃ©s.")
        rag_context = "\n\n".join(
            [f"ğŸ“„ Source: {r.metadata.get('source', 'inconnu')}:\n{r.page_content}" for r in results]
        )
    
    # Stocker le contexte RAG dans l'Ã©tat
    return {"rag_context": rag_context}


# ğŸ†• NÅ“ud 4 : SynthÃ¨se et Sauvegarde (RÃ©ponse Finale)
def answer_node(state: AgentState):
    """SynthÃ©tise la rÃ©ponse en utilisant le contexte RAG, l'historique, et sauvegarde l'Ã©change."""
    
    # 1. PrÃ©paration des contextes
    thread_id = state["thread_id"]
    user_query = state["user_query"]
    messages = state["messages"]
    history_context = state["history_context"] 
    # rag_context sera vide si rag_search_node n'a pas Ã©tÃ© appelÃ©
    rag_context = state.get("rag_context", "Aucun contexte documentaire supplÃ©mentaire n'a Ã©tÃ© jugÃ© nÃ©cessaire.")
    
    # 2. Construction du Prompt SystÃ¨me (AmÃ©liorÃ© pour gÃ©rer le cas RAG vide)
    system_prompt = (
        "Tu es un assistant RAG. RÃ©ponds de maniÃ¨re prÃ©cise et professionnelle. "
        "Si le Contexte RAG contient des informations pertinentes, utilise-les comme source principale. "
        "Sinon, utilise l'Historique de conversation pour conserver le contexte. "
        "Ne fais rÃ©fÃ©rence au 'Contexte RAG' que si tu l'utilises."
        "\n\n--- Contexte RAG ---\n"
        f"{rag_context}"
        "\n\n--- Historique de conversation (5 derniers Ã©changes) ---\n"
        f"{history_context}"
    )

    # 3. Appel du modÃ¨le
    system_message_obj = SystemMessage(content=system_prompt)

    # Le tableau final doit contenir uniquement des objets BaseMessage
    final_messages = [system_message_obj]

    # Ajouter le dernier message utilisateur (qui est dÃ©jÃ  un objet BaseMessage)
    final_messages.append(state["messages"][-1]) 

    response = llm.invoke(final_messages)
    text_response = getattr(response, "content", "")

    # 4. Sauvegarde dans Chroma (MÃ©moire conversationnelle)
    try:
        history_db.add_texts(
            texts=[f"USER: {user_query}\nAI: {text_response}"],
            metadatas=[{"thread_id": thread_id}],
        )
    except Exception as e:
        print(f"âš ï¸ Erreur lors de la sauvegarde de lâ€™historique : {e}")

    # 5. Retourner la rÃ©ponse
    return {"messages": [response]}


# =====================================================
# 4ï¸âƒ£ CrÃ©ation du Graphe LangGraph (Orchestration)
# =====================================================
checkpointer = InMemorySaver()
builder = StateGraph(AgentState)

# Ajouter les nÅ“uds
builder.add_node("history_retriever_node", history_retriever_node)
builder.add_node("rag_search_node", rag_search_node)
builder.add_node("answer_node", answer_node)

# DÃ©finir les chemins (Edges)
builder.add_edge(START, "history_retriever_node")
# Le nÅ“ud history_retriever_node passe le relais Ã  la fonction router_node
builder.add_conditional_edges(
    "history_retriever_node", # â¬…ï¸ DÃ©marre le routage aprÃ¨s le 'history_retriever_node'
    router_node,              # â¬…ï¸ Utilise la fonction router_node pour la dÃ©cision
    {
        "rag_search_node": "rag_search_node", # Si besoin de RAG -> Recherche
        "answer_node": "answer_node",         # Si pas besoin de RAG -> RÃ©ponse directe
    },
)

# AprÃ¨s la recherche RAG, on va toujours Ã  la rÃ©ponse
builder.add_edge("rag_search_node", "answer_node")

# Fin
builder.add_edge("answer_node", END)

# Compilation du graphe
agent_rag_graph = builder.compile(checkpointer=checkpointer)


# =====================================================
# 5ï¸âƒ£ Boucle interactive de l'Agent
# =====================================================
thread_id = "main_thread"
print("ğŸ¤– Agent RAG + Routage + MÃ©moire + LangGraph prÃªt !")
print("ğŸ’¬ Tapez votre question, ou 'history' pour voir les 5 derniers Ã©changes.\n")

while True:
    query = input("ğŸ§  Votre question : ").strip()

    # Sortie propre et commande 'history' (inchangÃ©es)
    if query.lower() in ["exit", "quit"]:
        print("\nğŸ‘‹ Fin de la session. Ã€ bientÃ´t !")
        break
    
    if query.lower() == "history":
        try:
            all_history = history_db.get(include=["metadatas", "documents"])
            thread_history = []
            for doc, meta in zip(all_history["documents"], all_history["metadatas"]):
                content = doc.get("page_content", doc) if isinstance(doc, dict) else str(doc)
                if meta.get("thread_id") == thread_id:
                    thread_history.append(content)

            if not thread_history:
                print("ğŸ•³ï¸ Aucun historique trouvÃ© pour ce thread.")
                continue

            print("\nğŸ“œ 5 derniers Ã©changes :\n")
            for entry in thread_history[-5:]:
                print(entry)
                print("-" * 40)
        except Exception as e:
            print(f"âš ï¸ Erreur lors de la rÃ©cupÃ©ration de lâ€™historique : {e}")
        continue


    # ğŸ†• PrÃ©paration de l'entrÃ©e pour le Graphe Agent
    # Le graphe reÃ§oit la question utilisateur, l'ID du thread et la query brute.
    initial_state = {
        "messages": [HumanMessage(content=query)], 
        "thread_id": thread_id,
        "user_query": query,
        "rag_context": "", # InitialisÃ© vide
        "history_context": "" # InitialisÃ© vide
    }

    # Appel du graphe
    print("ğŸ’­ L'Agent rÃ©flÃ©chit (Routage et GÃ©nÃ©ration)...")
    # 1. âš ï¸ DÃ©finir la configuration du Checkpointer
    checkpointer_config = {
        "configurable": {
            "thread_id": thread_id # Indique au Checkpointer oÃ¹ sauvegarder/charger
        }
    }
    try:
        # Lancement du graphe avec l'Ã©tat initial ET la configuration du checkpointer.
        output = agent_rag_graph.invoke(initial_state,config=checkpointer_config)

        # Extraction et affichage de la rÃ©ponse finale
        messages_out = output.get("messages", [])
        if not messages_out:
            print("âš ï¸ Aucune rÃ©ponse gÃ©nÃ©rÃ©e.")
            continue

        last_msg = messages_out[-1]
        response_text = getattr(last_msg, "content", None)

        if not response_text:
            print(f"âš ï¸ Contenu vide : {last_msg}")
        else:
            print("\nğŸ¤– RÃ©ponse :\n")
            print(response_text.strip(), "\n")

    except Exception as e:
        print(f"âŒ Erreur pendant la gÃ©nÃ©ration : {e}")