import os
from typing import Annotated, TypedDict
from dotenv import load_dotenv

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages, AnyMessage


# =====================================================
# 1Ô∏è‚É£ D√©finition de l‚Äô√©tat du graphe
# =====================================================
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    user_query: str  

# =====================================================
# 2Ô∏è‚É£ Configuration de base
# =====================================================
load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("‚ö†Ô∏è GOOGLE_API_KEY manquante dans le fichier .env")

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# M√©moire conversationnelle persistante (Chroma)
history_db = Chroma(
    persist_directory="chroma_history",
    embedding_function=embeddings,
)

# Base documentaire RAG
vectordb = Chroma(
    persist_directory="chroma_db",
    embedding_function=embeddings,
)

# Mod√®le LLM Gemini
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0.3,
    max_tokens=700,
    timeout=None,
    max_retries=2,
)


# =====================================================
# 3Ô∏è‚É£ N≈ìud principal : g√©n√©ration via le mod√®le
# =====================================================
def call_model(state: State):
    messages = state["messages"]
    user_query = state.get("user_query","")


    # üìú Charger les 5 derniers √©changes pour ce thread
    thread_history = []
    try:
        all_history = history_db.get(include=["metadatas", "documents"])
        for doc, meta in zip(all_history["documents"], all_history["metadatas"]):
            if meta.get("thread_id") == thread_id:
                content = getattr(doc, "page_content", doc)
                thread_history.append(content)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration de l‚Äôhistorique : {e}")

    last_messages = thread_history[-5:]
    history_context = "\n".join(last_messages)

    # Construire les messages pour le LLM
    contextual_messages = [
        {
            "role": "system",
            "content": (
                "Tu es un assistant RAG. R√©ponds uniquement √† partir des extraits fournis "
                "et de l‚Äôhistorique suivant :\n"
                f"{history_context}\n"
            ),
        }
    ]

    for m in messages:
        role = getattr(m, "role", getattr(m, "type", "user"))
        content = getattr(m, "content", "")
        contextual_messages.append({"role": role, "content": content})

    # Appel du mod√®le
    response = llm.invoke(contextual_messages)

    # Extraire le texte
    text_response = getattr(response, "content", "")
    if isinstance(text_response, list):
        text_response = " ".join([c.get("text", "") for c in text_response if isinstance(c, dict)])

    # Sauvegarder dans Chroma
    history_db.add_texts(
        texts=[f"USER: {user_query}\nAI: {text_response}"],
        metadatas=[{"thread_id": thread_id}],
    )

    return {"messages": [response]}




# =====================================================
# 4Ô∏è‚É£ Cr√©ation du graphe LangGraph
# =====================================================
checkpointer = InMemorySaver()
builder = StateGraph(State)
builder.add_node("model", call_model)
builder.add_edge(START, "model")
builder.add_edge("model", END)
graph = builder.compile(checkpointer=checkpointer)


# =====================================================
# 5Ô∏è‚É£ Boucle interactive
# =====================================================
thread_id = "main_thread"
print("ü§ñ Assistant RAG + M√©moire + LangGraph pr√™t !")
print("üí¨ Tapez votre question, ou 'history' pour voir les 5 derniers √©changes.\n")

while True:
    query = input("üß† Votre question : ").strip()

    # Sortie propre
    if query.lower() in ["exit", "quit"]:
        print("\nüëã Fin de la session. √Ä bient√¥t !")
        break

    # Commande sp√©ciale : afficher l‚Äôhistorique
    if query.lower() == "history":
        try:
            all_history = history_db.get(include=["metadatas", "documents"])
            thread_history = []
            for doc, meta in zip(all_history["documents"], all_history["metadatas"]):
                if meta.get("thread_id") == thread_id:
                    # doc peut √™tre un str ou un Document
                    content = getattr(doc, "page_content", doc)
                    thread_history.append(content)

            if not thread_history:
                print("üï≥Ô∏è Aucun historique trouv√© pour ce thread.")
                continue

            print("\nüìú 5 derniers √©changes :\n")
            for entry in thread_history[-5:]:
                print(entry)
                print("-" * 40)

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration de l‚Äôhistorique : {e}")
        continue


    # Recherche vectorielle RAG
    results = vectordb.similarity_search(query, k=5)
    if not results:
        print("‚ùå Aucun r√©sultat pertinent trouv√© dans la base.")
        continue

    # Contexte documentaire
    rag_context = "\n\n".join(
        [f"üìÑ {r.metadata.get('source', 'inconnu')}:\n{r.page_content}" for r in results]
    )

    # Construire le prompt pour le mod√®le
    messages = [
        {
            "role": "system",
            "content": (
                "Tu es un assistant RAG. R√©ponds uniquement √† partir des extraits fournis "
                "et utilise l‚Äôhistorique conversationnel pour conserver le contexte."
            ),
        },
        {"role": "user", "content": f"Contexte documentaire :\n{rag_context}\n\nQuestion : {query}"},
    ]

    # Appel du graphe
    print("üí≠ G√©n√©ration de la r√©ponse...")
    try:
        output = graph.invoke(
            {"messages": messages,"user_query": query},
            config={"configurable": {"thread_id": thread_id}},
        )

        messages_out = output.get("messages", [])
        if not messages_out:
            print("‚ö†Ô∏è Aucune r√©ponse g√©n√©r√©e.")
            continue

        last_msg = messages_out[-1]
        response_text = getattr(last_msg, "content", None)

        if not response_text:
            print(f"‚ö†Ô∏è Contenu vide : {last_msg}")
        else:
            print("\nü§ñ R√©ponse :\n")
            print(response_text.strip(), "\n")

    except Exception as e:
        print(f"‚ùå Erreur pendant la g√©n√©ration : {e}")
