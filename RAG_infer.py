import os
from typing import Annotated, TypedDict

from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages, AnyMessage


# =====================================================
# 1Ô∏è‚É£ D√©finition de l‚Äô√©tat du graphe
# =====================================================
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]


# =====================================================
# 2Ô∏è‚É£ Configuration de base
# =====================================================
load_dotenv()
if not os.environ.get("GOOGLE_API_KEY"):
    raise ValueError("‚ö†Ô∏è GOOGLE_API_KEY manquante dans le fichier .env")

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
vectordb = Chroma(persist_directory="chroma_db", embedding_function=embeddings)

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0.3,
    max_tokens=700,
    timeout=None,
    max_retries=2
)


# =====================================================
# 3Ô∏è‚É£ D√©finition du n≈ìud de g√©n√©ration
# =====================================================
def call_model(state: State):
    """N≈ìud : appelle Gemini avec l‚Äôhistorique et renvoie la r√©ponse."""
    messages = state["messages"]
    response = llm.invoke(messages)
    return {"messages": [response]}


# =====================================================
# 4Ô∏è‚É£ Cr√©ation et compilation du graphe LangGraph
# =====================================================
checkpointer = InMemorySaver()
builder = StateGraph(State)

# Ajoute ton n≈ìud principal
builder.add_node("model", call_model)

# üß† d√©finir le flux
builder.add_edge(START, "model")  # entr√©e ‚Üí model
builder.add_edge("model", END)    # model ‚Üí fin

# Compilation
graph = builder.compile(checkpointer=checkpointer)


# =====================================================
# 5Ô∏è‚É£ Boucle interactive avec m√©moire persistante
# =====================================================
thread_id = "main_thread"
print("ü§ñ Assistant RAG + LangGraph pr√™t ! (tape 'exit' ou 'quit' pour arr√™ter)\n")

while True:
    query = input("üß† Votre question : ").strip()
    if query.lower() in ["exit", "quit"]:
        print("\nüëã Fin de la session. √Ä bient√¥t !")
        break

    # Recherche vectorielle RAG
    results = vectordb.similarity_search(query, k=5)
    if not results:
        print("‚ùå Aucun r√©sultat pertinent trouv√© dans la base.")
        continue

    # Construire le contexte √† partir des extraits trouv√©s
    rag_context = "\n\n".join(
        [f"üìÑ {r.metadata.get('source', 'inconnu')}:\n{r.page_content}" for r in results]
    )

    # Construire le prompt pour le LLM
    messages = [
        {"role": "system", "content": "Tu es un assistant RAG. R√©ponds uniquement √† partir des extraits fournis."},
        {"role": "user", "content": f"Contexte :\n{rag_context}\n\nQuestion : {query}"}
    ]

    # Appel du graphe avec historique LangGraph
    print("üí≠ G√©n√©ration de la r√©ponse...")
    try:
        output = graph.invoke(
            {"messages": messages},
            config={"configurable": {"thread_id": thread_id}},
        )

        # V√©rification de la structure de la sortie
        messages_out = output.get("messages", [])
        if not messages_out:
            print("‚ö†Ô∏è Aucune r√©ponse g√©n√©r√©e par le mod√®le.")
            continue

        last_msg = messages_out[-1]
        response_text = getattr(last_msg, "content", None)

        if not response_text:
            print(f"‚ö†Ô∏è Contenu vide. Message brut : {last_msg}")
        else:
            print("\nü§ñ R√©ponse :\n")
            print(response_text.strip(), "\n")

    except Exception as e:
        print(f"‚ùå Erreur pendant la g√©n√©ration : {e}")